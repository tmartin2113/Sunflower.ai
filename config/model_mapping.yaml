# Sunflower AI Professional System - Model Mapping Configuration
# Maps hardware capabilities to optimal AI models
# Ensures best performance across all hardware configurations

# ============================================================================
# HARDWARE TIER DEFINITIONS
# ============================================================================

version: "1.0.0"
auto_detect: true

hardware_tiers:
  # High-end systems (gaming PCs, workstations)
  high_end:
    tier_name: "Performance"
    tier_priority: 1  # Highest priority
    
    # Hardware requirements
    requirements:
      min_ram_gb: 16
      min_vram_gb: 8
      min_cpu_cores: 8
      min_cpu_speed_ghz: 3.0
      architecture: ["x86_64", "arm64"]
      gpu_compute_capability: 6.0  # CUDA compute capability
      
    # Model configuration
    model_config:
      base_model: "llama3.2:7b"
      model_path: "models/llama3.2-7b.gguf"
      model_size_gb: 4.5
      
      # Inference settings
      context_size: 4096
      batch_size: 512
      threads: 8
      gpu_layers: 35  # Number of layers to offload to GPU
      use_mmap: true
      use_mlock: false
      
      # Memory allocation
      max_memory_mb: 8192
      gpu_memory_fraction: 0.9
      
      # Performance optimizations
      use_flash_attention: true
      use_quantization: false
      precision: "fp16"
      
    # Response characteristics
    response_config:
      max_tokens: 2048
      temperature: 0.7
      top_p: 0.95
      top_k: 40
      repeat_penalty: 1.1
      presence_penalty: 0.0
      frequency_penalty: 0.0
      
    # Quality settings
    quality_settings:
      response_quality: "highest"
      response_speed: "fast"
      accuracy_priority: 0.9
      creativity_level: 0.7
      
  # Mid-range systems (modern laptops, older gaming PCs)
  mid_range:
    tier_name: "Balanced"
    tier_priority: 2
    
    requirements:
      min_ram_gb: 8
      min_vram_gb: 4
      min_cpu_cores: 4
      min_cpu_speed_ghz: 2.5
      architecture: ["x86_64", "arm64"]
      gpu_compute_capability: 5.0
      
    model_config:
      base_model: "llama3.2:3b"
      model_path: "models/llama3.2-3b.gguf"
      model_size_gb: 2.0
      
      context_size: 2048
      batch_size: 256
      threads: 4
      gpu_layers: 24
      use_mmap: true
      use_mlock: false
      
      max_memory_mb: 4096
      gpu_memory_fraction: 0.8
      
      use_flash_attention: true
      use_quantization: false
      precision: "fp16"
      
    response_config:
      max_tokens: 1024
      temperature: 0.7
      top_p: 0.9
      top_k: 40
      repeat_penalty: 1.1
      presence_penalty: 0.0
      frequency_penalty: 0.0
      
    quality_settings:
      response_quality: "high"
      response_speed: "balanced"
      accuracy_priority: 0.85
      creativity_level: 0.7
      
  # Low-end systems (basic laptops, older computers)
  low_end:
    tier_name: "Efficient"
    tier_priority: 3
    
    requirements:
      min_ram_gb: 4
      min_vram_gb: 0  # CPU-only inference
      min_cpu_cores: 2
      min_cpu_speed_ghz: 2.0
      architecture: ["x86_64", "arm64", "x86"]
      gpu_compute_capability: 0  # No GPU required
      
    model_config:
      base_model: "llama3.2:1b"
      model_path: "models/llama3.2-1b.gguf"
      model_size_gb: 0.7
      
      context_size: 1024
      batch_size: 128
      threads: 2
      gpu_layers: 0  # CPU-only
      use_mmap: true
      use_mlock: false
      
      max_memory_mb: 2048
      gpu_memory_fraction: 0.0
      
      use_flash_attention: false
      use_quantization: true
      precision: "int8"
      
    response_config:
      max_tokens: 512
      temperature: 0.7
      top_p: 0.9
      top_k: 30
      repeat_penalty: 1.1
      presence_penalty: 0.0
      frequency_penalty: 0.0
      
    quality_settings:
      response_quality: "good"
      response_speed: "responsive"
      accuracy_priority: 0.8
      creativity_level: 0.6
      
  # Minimum spec systems (very old or limited hardware)
  minimum:
    tier_name: "Compatibility"
    tier_priority: 4  # Lowest priority
    
    requirements:
      min_ram_gb: 2
      min_vram_gb: 0
      min_cpu_cores: 1
      min_cpu_speed_ghz: 1.5
      architecture: ["x86_64", "arm64", "x86", "arm"]
      gpu_compute_capability: 0
      
    model_config:
      base_model: "llama3.2:1b-q4_0"
      model_path: "models/llama3.2-1b-q4_0.gguf"
      model_size_gb: 0.4
      
      context_size: 512
      batch_size: 64
      threads: 1
      gpu_layers: 0
      use_mmap: false  # Reduce memory usage
      use_mlock: false
      
      max_memory_mb: 1024
      gpu_memory_fraction: 0.0
      
      use_flash_attention: false
      use_quantization: true
      precision: "int4"  # 4-bit quantization
      
    response_config:
      max_tokens: 256
      temperature: 0.7
      top_p: 0.85
      top_k: 20
      repeat_penalty: 1.15
      presence_penalty: 0.0
      frequency_penalty: 0.0
      
    quality_settings:
      response_quality: "acceptable"
      response_speed: "priority"
      accuracy_priority: 0.75
      creativity_level: 0.5

# ============================================================================
# MODEL ALIASES AND CONFIGURATIONS
# ============================================================================

model_aliases:
  # Sunflower Kids model (primary interface for children)
  sunflower-kids:
    display_name: "Sunflower AI Kids"
    description: "Safe, age-adaptive STEM education for children"
    
    # Base model selection
    base_model: "auto"  # Automatically selected based on hardware tier
    modelfile: "modelfiles/Sunflower_AI_Kids.modelfile"
    
    # Model-specific parameters
    parameters:
      temperature: 0.7
      top_p: 0.9
      top_k: 40
      repeat_penalty: 1.1
      seed: -1  # Random seed
      stop:
        - "</s>"
        - "[INST]"
        - "[/INST]"
        - "Human:"
        - "Assistant:"
        
    # Safety configuration
    safety:
      content_filter: "maximum"
      topic_restrictions: true
      personal_info_protection: true
      response_sanitization: true
      
    # Age-based adjustments
    age_adjustments:
      enabled: true
      auto_detect_from_conversation: true
      fallback_age: 10
      
    # Caching
    cache_config:
      cache_responses: true
      cache_ttl_seconds: 3600
      max_cache_size_mb: 100
      
  # Sunflower Educator model (for parents and teachers)
  sunflower-educator:
    display_name: "Sunflower AI Educator"
    description: "Professional STEM education assistant for adults"
    
    base_model: "auto"
    modelfile: "modelfiles/Sunflower_AI_Educator.modelfile"
    
    parameters:
      temperature: 0.8
      top_p: 0.95
      top_k: 50
      repeat_penalty: 1.0
      seed: -1
      stop:
        - "</s>"
        - "[INST]"
        - "[/INST]"
        - "Human:"
        - "Assistant:"
        
    safety:
      content_filter: "minimal"
      topic_restrictions: false
      personal_info_protection: true
      response_sanitization: false
      
    age_adjustments:
      enabled: false
      
    cache_config:
      cache_responses: false
      cache_ttl_seconds: 0
      max_cache_size_mb: 0

# ============================================================================
# HARDWARE DETECTION CONFIGURATION
# ============================================================================

hardware_detection:
  # Detection method priority
  detection_methods:
    - "native"      # Platform-specific APIs
    - "wmi"         # Windows Management Instrumentation (Windows)
    - "sysctl"      # System control (macOS)
    - "proc"        # /proc filesystem (Linux)
    - "command"     # Shell commands
    - "fallback"    # Conservative estimates
    
  # Detection timeouts
  detection_timeout_seconds: 10
  gpu_detection_timeout_seconds: 5
  
  # Caching
  cache_hardware_info: true
  cache_duration_seconds: 3600
  
  # GPU detection
  gpu_detection:
    detect_nvidia: true
    detect_amd: true
    detect_intel: true
    detect_apple: true  # Apple Silicon
    
    # CUDA detection (NVIDIA)
    cuda_detection_enabled: true
    min_cuda_version: "11.0"
    
    # ROCm detection (AMD)
    rocm_detection_enabled: true
    min_rocm_version: "5.0"
    
    # Metal detection (Apple)
    metal_detection_enabled: true
    
  # Platform-specific settings
  platform_specific:
    windows:
      use_wmi: true
      use_dxdiag: false
      detect_wsl: true
      
    macos:
      use_sysctl: true
      use_system_profiler: true
      detect_rosetta: true
      
    linux:
      use_proc: true
      use_lshw: false
      detect_container: true

# ============================================================================
# PERFORMANCE OPTIMIZATION
# ============================================================================

performance_optimization:
  # Automatic optimization
  auto_optimize: true
  optimization_level: "balanced"  # aggressive, balanced, conservative
  
  # CPU optimization
  cpu_optimization:
    auto_detect_cores: true
    reserve_cores_for_system: 1
    use_performance_cores: true  # For hybrid CPUs
    enable_avx: true
    enable_avx2: true
    enable_avx512: false  # Often disabled due to throttling
    
  # Memory optimization
  memory_optimization:
    use_huge_pages: false
    memory_pool_size_mb: 512
    aggressive_gc: false
    preallocate_memory: true
    
  # GPU optimization
  gpu_optimization:
    prefer_gpu: true
    multi_gpu_enabled: false
    gpu_scheduling: "exclusive"  # exclusive, shared
    power_limit_percent: 100
    
  # Model loading optimization
  model_loading:
    preload_models: true
    lazy_loading: false
    model_warmup: true
    warmup_iterations: 3
    
  # Response streaming
  streaming:
    enabled: true
    chunk_size_tokens: 10
    buffer_size_kb: 64
    flush_interval_ms: 100

# ============================================================================
# FALLBACK CONFIGURATION
# ============================================================================

fallback_config:
  # Fallback model when auto-detection fails
  default_model: "llama3.2:1b-q4_0"
  default_tier: "minimum"
  
  # Fallback parameters
  fallback_parameters:
    context_size: 512
    max_tokens: 256
    threads: 1
    gpu_layers: 0
    
  # Error handling
  on_model_load_failure: "use_fallback"  # use_fallback, retry, error
  max_retry_attempts: 3
  retry_delay_seconds: 5
  
  # Degradation strategy
  graceful_degradation:
    enabled: true
    degradation_steps:
      - "reduce_context_size"
      - "reduce_gpu_layers"
      - "switch_to_smaller_model"
      - "use_quantized_model"
      - "use_minimum_tier"

# ============================================================================
# MONITORING AND METRICS
# ============================================================================

monitoring:
  # Performance metrics
  track_inference_time: true
  track_token_generation_rate: true
  track_memory_usage: true
  track_gpu_utilization: true
  
  # Quality metrics
  track_response_quality: false  # Requires user feedback
  track_safety_incidents: true
  track_cache_hit_rate: true
  
  # Thresholds for alerts
  performance_thresholds:
    max_inference_time_seconds: 10
    min_tokens_per_second: 5
    max_memory_usage_percent: 90
    max_gpu_memory_percent: 95
    
  # Logging
  log_performance_metrics: true
  metrics_log_interval_seconds: 300
  detailed_logging: false

# End of model mapping configuration
